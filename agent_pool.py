import asyncio
import heapq
import logging
from time import time
from typing import Any, Callable, Coroutine, List, Optional
import uuid
import openai

logger = logging.getLogger(__name__)

# ----------------------------------------------------------------------
# ä¼˜å…ˆçº§é˜Ÿåˆ—æ ¸å¿ƒï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ----------------------------------------------------------------------
class Task:
    __slots__ = ('priority', 'data', 'timestamp', 'order', 'task_id', 'future')

    def __init__(self, priority: int, data: Any, future: Optional[asyncio.Future] = None):
        if not 0 <= priority <= 15:
            raise ValueError("priority must be 0-15")
        self.priority = priority
        self.data = data
        self.timestamp = time()
        self.order = 0
        self.task_id = uuid.uuid4().hex[:8]
        self.future = future

    def __lt__(self, other: 'Task') -> bool:
        if self.priority != other.priority:
            return self.priority < other.priority
        return self.order < other.order


class PriorityScheduler:
    def __init__(self, maxsize: int = 0):
        self.maxsize = maxsize
        self._queue: list[Task] = []
        self._counter = 0
        self._lock = asyncio.Lock()
        self._not_empty = asyncio.Condition(self._lock)

    async def put(self, task: Task) -> None:
        async with self._not_empty:
            if self.maxsize > 0 and len(self._queue) >= self.maxsize:
                raise asyncio.QueueFull()
            task.order = self._counter
            self._counter += 1
            heapq.heappush(self._queue, task)
            self._not_empty.notify()

    async def pop(self) -> Task:
        async with self._not_empty:
            while not self._queue:
                await self._not_empty.wait()
            return heapq.heappop(self._queue)

    def qsize(self): return len(self._queue)
    def empty(self): return len(self._queue) == 0
    def full(self): return self.maxsize > 0 and len(self._queue) >= self.maxsize


# ----------------------------------------------------------------------
# å…¨å±€çŠ¶æ€
# ----------------------------------------------------------------------
_scheduler: Optional[PriorityScheduler] = None
_loop: Optional[asyncio.AbstractEventLoop] = None
_worker_tasks: List[asyncio.Task] = []

# LLM å¤„ç†å™¨ï¼ˆé»˜è®¤ä¸º Noneï¼Œå¯åŠ¨åå¿…é¡»æ³¨å†Œï¼‰
_llm_handler: Optional[Callable[[Any], Coroutine]] = None


# ----------------------------------------------------------------------
# æ³¨å†Œæ¥å£ï¼ˆè£…é¥°å™¨ + å‡½æ•°ï¼‰
# ----------------------------------------------------------------------
def register_llm_handler(func: Optional[Callable[[Any], Coroutine]] = None):
    """
    è£…é¥°å™¨ï¼šæ³¨å†Œ LLM å¤„ç†å™¨ã€‚
    ç”¨æ³•ï¼š
        @register_llm_handler
        async def my_llm_handler(data): ...
    æˆ–ï¼š
        register_llm_handler(my_llm_handler)
    """
    def decorator(f: Callable[[Any], Coroutine]) -> Callable[[Any], Coroutine]:
        global _llm_handler
        _llm_handler = f
        logger.info("LLM å¤„ç†å™¨å·²æ³¨å†Œ: %s", f.__name__)
        return f

    if func is None:
        return decorator
    else:
        return decorator(func)


# ----------------------------------------------------------------------
# Worker æ± ï¼ˆå›ºå®šæ•°é‡ï¼Œä¸²è¡Œæ‰§è¡Œï¼‰
# ----------------------------------------------------------------------
async def _worker(worker_id: int):
    while True:
        task = None
        try:
            task = await _scheduler.pop()
            if _llm_handler is None:
                err = RuntimeError("LLM å¤„ç†å™¨æœªæ³¨å†Œï¼Œè¯·å…ˆè°ƒç”¨ register_llm_handler")
                logger.error(err)
                if task.future:
                    task.future.set_exception(err)
                continue

            try:
                result = await _llm_handler(task.data)
                if task.future and not task.future.done():
                    task.future.set_result(result)
            except Exception as e:
                logger.exception("Worker-%d å¤„ç†ä»»åŠ¡å¤±è´¥, task_id=%s", worker_id, task.task_id)
                if task.future and not task.future.done():
                    task.future.set_exception(e)

        except asyncio.CancelledError:
            logger.info("Worker-%d å·²å–æ¶ˆ", worker_id)
            break
        except Exception:
            logger.exception("Worker-%d å†…éƒ¨å¼‚å¸¸", worker_id)


# ----------------------------------------------------------------------
# å¯åŠ¨ / åœæ­¢
# ----------------------------------------------------------------------
async def setup_agent_pool(
    worker_count: int = 5,
    maxsize: int = 100,
    loop: Optional[asyncio.AbstractEventLoop] = None
) -> None:
    """å¯åŠ¨ä»£ç†æ± ï¼ˆåªéœ€è¦ Worker æ•°é‡å’Œé˜Ÿåˆ—å®¹é‡ï¼‰"""
    global _scheduler, _loop, _worker_tasks

    if _scheduler is not None:
        logger.warning("Agent æ± å·²å¯åŠ¨ï¼Œå¿½ç•¥é‡å¤è°ƒç”¨")
        return

    _scheduler = PriorityScheduler(maxsize=maxsize)
    _loop = loop or asyncio.get_running_loop()
    _worker_tasks = [
        asyncio.create_task(_worker(i), name=f"AgentWorker-{i}")
        for i in range(worker_count)
    ]
    logger.info("âœ… Agent æ± å·²å¯åŠ¨ï¼ŒWorker æ•°é‡=%dï¼Œé˜Ÿåˆ—å®¹é‡=%s",
                worker_count, maxsize if maxsize > 0 else "æ— é™åˆ¶")


async def stop_agent_pool() -> None:
    """åœæ­¢ä»£ç†æ± """
    global _worker_tasks, _scheduler, _loop, _llm_handler
    for t in _worker_tasks:
        t.cancel()
    if _worker_tasks:
        await asyncio.gather(*_worker_tasks, return_exceptions=True)
    _worker_tasks.clear()
    _scheduler = None
    _loop = None
    _llm_handler = None
    logger.info("ğŸ›‘ Agent æ± å·²åœæ­¢")


# ----------------------------------------------------------------------
# å¯¹å¤–è°ƒç”¨æ¥å£ï¼šawait agentp_LLM(...) ç›´æ¥å¾—åˆ°å›å¤
# ----------------------------------------------------------------------
async def agentp_LLM(
    api_key: str,
    prompt: str,
    api_base: Optional[str] = None,
    model: str = "gpt-3.5-turbo",
    priority: int = 7,
    timeout: float = 30.0,
    **kwargs
) -> str:
    """
    å¼‚æ­¥æäº¤ LLM è¯·æ±‚ï¼Œç­‰å¾…å›å¤ã€‚
    å‚æ•°ä¼šè¢«æ‰“åŒ…æˆå­—å…¸ä¼ é€’ç»™å·²æ³¨å†Œçš„ LLM å¤„ç†å™¨ã€‚
    å¤„ç†å™¨å¿…é¡»è¿”å›å­—ç¬¦ä¸²ï¼ˆæ¨¡å‹å›å¤ï¼‰ã€‚
    """
    if _scheduler is None:
        raise RuntimeError("âŒ Agent æ± æœªå¯åŠ¨ï¼Œè¯·å…ˆè°ƒç”¨ setup_agent_pool()")

    request_data = {
        "api_key": api_key,
        "api_base": api_base,
        "prompt": prompt,
        "model": model,
        **kwargs
    }

    loop = _loop or asyncio.get_running_loop()
    future = loop.create_future()
    task = Task(priority=priority, data=request_data, future=future)

    try:
        await _scheduler.put(task)
    except asyncio.QueueFull:
        raise RuntimeError("Agent æ± é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·æ±‚è¢«æ‹’ç»")

    try:
        return await asyncio.wait_for(future, timeout=timeout)
    except asyncio.TimeoutError:
        if not future.done():
            future.cancel()
        raise


# ----------------------------------------------------------------------
# å…¬å¼€æ¥å£
# ----------------------------------------------------------------------
__all__ = [
    "setup_agent_pool",
    "stop_agent_pool",
    "agentp_LLM",
    "register_llm_handler",
]


@register_llm_handler
async def openai_handler(data):
    """ä» data ä¸­æå–å‚æ•°ï¼Œè°ƒç”¨ OpenAIï¼Œè¿”å›å­—ç¬¦ä¸²"""
    openai.api_key = data["api_key"]
    if data.get("api_base"):
        openai.api_base = data["api_base"]

    response = await openai.ChatCompletion.acreate(
        model=data.get("model", "gpt-3.5-turbo"),
        messages=[{"role": "user", "content": data["prompt"]}],
        temperature=data.get("temperature", 0.7),
        max_tokens=data.get("max_tokens", 150)
    )
    return response.choices[0].message.content